{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e266cd-aea1-48cd-9e76-ef9ccbdbbb45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-process and write to npz for GNN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89486a71-24e9-4545-9df6-b61188d58a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "re_int_sci = r'[-\\d\\.]+e?[-+\\d]*'\n",
    "re_sci = r'[+-]?\\d+\\.\\d+e[+-]?[\\d]+'\n",
    "\n",
    "def parse_fragment_simulation(file):\n",
    "    '''\n",
    "    Extract info from LSDYNA txt, including particle coordinates, particle types, and effective plastic strain (eps).\"\n",
    "    Input: Txt from LYDYNA, e.g., C_80_480_Cc_20_strain.txt\n",
    "    Output: np arrays of shapes, \n",
    "            tracjectory (timesteps, num_particles, 3), particle_type (num_particles,), eps (timesteps, num_particles).\n",
    "    '''\n",
    "                   \n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines() \n",
    "\n",
    "    # Find all \"particle position\" lines and \"plastic strain\" lines using key words\n",
    "    pos_lines_start, pos_lines_end = [], []\n",
    "    strain_lines_start, strain_lines_end = [], []\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"*NODE\"):\n",
    "            pos_lines_start.append(idx)\n",
    "        elif line.startswith(\"*END\"):  \n",
    "            pos_lines_end.append(idx)\n",
    "        elif line.startswith(\"$SPH_ELEMENT_RESULTS\"):\n",
    "            strain_lines_start.append(idx)\n",
    "\n",
    "    num_steps = len(pos_lines_start)\n",
    "    pos_lines_end, strain_lines_end = pos_lines_end[:num_steps], pos_lines_end[num_steps:]\n",
    "\n",
    "    # Extact particle types\n",
    "    type_line_start = pos_lines_start[0]\n",
    "    type_line_end = pos_lines_end[0]\n",
    "    particle_types = []\n",
    "    eids = []\n",
    "    for line in lines[type_line_start:type_line_end]:\n",
    "        num_str = re.findall(re_int_sci, line)  # Regular expression findign integers\n",
    "        if len(num_str) == 4:\n",
    "            eid = int(num_str[0])\n",
    "            particle_type = 1 if eid < 263914 else 0\n",
    "            particle_types.append(particle_type)\n",
    "    particle_types = np.array(particle_types).astype(int)\n",
    "\n",
    "    # Extact particle positions \n",
    "    trajectory = []\n",
    "    for line_start, line_end in zip(pos_lines_start, pos_lines_end):\n",
    "        pos_lines = lines[line_start:line_end]   # lines that contains positions in one time step\n",
    "        pos_one_step = []\n",
    "        for line in pos_lines:\n",
    "            num_str = re.findall(re_sci, line)  # Regular expression findign scitific numbers\n",
    "            if len(num_str) == 3:\n",
    "                pos = [float(x) for x in num_str] #last one is volume\n",
    "                pos = tuple(pos)\n",
    "                pos_one_step.append(pos)\n",
    "        trajectory.append(pos_one_step) \n",
    "    trajectory = np.array(trajectory).astype(float)\n",
    "\n",
    "    # Extract effective plastic strain (eps)\n",
    "    strains = []\n",
    "    for line_start, line_end in zip(strain_lines_start, strain_lines_end):\n",
    "        strain_lines = lines[line_start+1:line_end]   # lines that contains positions in one time step\n",
    "        strains_one_step = []\n",
    "        for line in strain_lines:\n",
    "            num_str = re.findall(re_sci, line)  # Regular expression findign scitific numbers\n",
    "            if len(num_str) == 1:\n",
    "                num = float(num_str[0]) \n",
    "                strains_one_step.append(num)\n",
    "        strains_one_step = [0]*6248 + strains_one_step   # Add zero strain to all beam particles\n",
    "        strains.append(strains_one_step)  \n",
    "    strains = np.array(strains).astype(float)\n",
    "    \n",
    "    return trajectory, particle_types, strains\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244aa8fe-0913-4d4b-92f4-c74698cbf200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/45 Reading /home/jovyan/work/data_temp/fragment/120/120_2_0.3C50...\n",
      "Dim: 3\n",
      "Position min:[-498.66158   -995.           3.2785579], max:[497.87956 995.      201.36161]\n",
      "Strain min:-1.3457093040134123, max:1.6891136082034723\n",
      "Shape, pos: (41, 246248, 3), types: (246248,), strain: (41, 246248)\n",
      "Unique particle types: [0 1]\n",
      "to train\n",
      "1/45 Reading /home/jovyan/work/data_temp/fragment/120/120_3_0.5C30...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, simulation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(simulations):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trajectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimulation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     positions, particle_types, strains \u001b[38;5;241m=\u001b[39m \u001b[43mparse_fragment_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     dim \u001b[38;5;241m=\u001b[39m positions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     40\u001b[0m     positions \u001b[38;5;241m=\u001b[39m positions[\u001b[38;5;241m20\u001b[39m::STEP_SIZE, :, :]\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mparse_fragment_simulation\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     37\u001b[0m eids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines[type_line_start:type_line_end]:\n\u001b[0;32m---> 39\u001b[0m     num_str \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre_int_sci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Regular expression findign integers\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_str) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     41\u001b[0m         eid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_str[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/mgn/lib/python3.9/re.py:241\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pathlib\n",
    "       \n",
    "\n",
    "dataset = 'Fragment'\n",
    "in_dir = f'/home/jovyan/work/data_temp/fragment/120/'\n",
    "out_dir = f'/home/jovyan/work/data_temp/fragment/{dataset}/'\n",
    "pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "strain_mean, strain_std = 0.8868453123315391, 0.6590170029193022\n",
    "\n",
    "# Grab all simulation cases from corresponding data folder\n",
    "simulations = glob.glob(in_dir +'*')\n",
    "random.shuffle(simulations)\n",
    "\n",
    "## Larger step size leads to shorter trajectory and hence better rollout performance\n",
    "## But lower precision of the simulation\n",
    "## Current simulation are of absolute time 30 ms\n",
    "## Step size=1 means 53 steps, each of which 0.6 ms\n",
    "STEP_SIZE = 2\n",
    "\n",
    "## Initialisation placeholders for data\n",
    "n_trajectory = len(simulations)\n",
    "ds_train, ds_valid, ds_test = {}, {}, {}\n",
    "vels = np.array([]).reshape(0, 3)\n",
    "accs = np.array([]).reshape(0, 3)\n",
    "strain_stats = np.array([])\n",
    "file_train, file_valid, file_test = [], [], []\n",
    "\n",
    "## Main loop for data extraction\n",
    "for idx, simulation in enumerate(simulations):\n",
    "    print(f\"{idx}/{n_trajectory} Reading {simulation}...\")\n",
    "    positions, particle_types, strains = parse_fragment_simulation(simulation)\n",
    "    dim = positions.shape[-1]\n",
    "    \n",
    "    positions = positions[20::STEP_SIZE, :, :]\n",
    "    \n",
    "    strains = strains[20::STEP_SIZE, :]\n",
    "    strains = (strains - strain_mean) / strain_std   ## standardize based on overall mean and std\n",
    "       \n",
    "    # print for debug\n",
    "    print(f\"Dim: {dim}\")\n",
    "    print(f\"Position min:{positions.min(axis=(0,1))}, max:{positions.max(axis=(0,1))}\")\n",
    "    print(f\"Strain min:{strains.min(axis=(0,1))}, max:{strains.max(axis=(0,1))}\")\n",
    "    print(f\"Shape, pos: {positions.shape}, types: {particle_types.shape}, strain: {strains.shape}\")\n",
    "    print(f\"Unique particle types: {np.unique(particle_types)}\")\n",
    "    \n",
    "    # Data splits: train(80%), valid(10%), test(10%)\n",
    "    key = f'trajectory_{idx}' \n",
    "    if idx < 40:\n",
    "        print('to train')\n",
    "        ds_train[key] = [positions, particle_types, strains]\n",
    "        file_train.append(simulation)\n",
    "    if idx >= 40:\n",
    "        print('to valid')\n",
    "        ds_valid[key] = [positions, particle_types, strains]\n",
    "        file_valid.append(simulation)\n",
    "    if idx >= 40:\n",
    "        print('to test')\n",
    "        ds_test[key] = [positions, particle_types, strains]\n",
    "        file_test.append(simulation)\n",
    "        \n",
    "    # Extract Vel and Acc statistics\n",
    "    # positions of shape [timestep, particles, dimensions]\n",
    "    vel_trajectory = positions[1:,:,:] - positions[:-1,:,:]\n",
    "    acc_trajectory = vel_trajectory[1:,:,:]- vel_trajectory[:-1,:,:]\n",
    "    \n",
    "    vels = np.concatenate((vels, vel_trajectory.reshape(-1, dim)), axis=0)\n",
    "    accs = np.concatenate((accs, acc_trajectory.reshape(-1, dim)), axis=0)\n",
    "\n",
    "# Extract vel, acc statistics for normalisation\n",
    "vel_mean, vel_std = list(vels.mean(axis=0)), list(vels.std(axis=0))\n",
    "acc_mean, acc_std = list(accs.mean(axis=0)), list(accs.std(axis=0))\n",
    "\n",
    "# # Save datasets in numpy format\n",
    "# np.savez(out_dir + 'train.npz', **ds_train)\n",
    "# np.savez(out_dir + 'valid.npz', **ds_valid)\n",
    "# np.savez(out_dir + 'test.npz', **ds_test)\n",
    "\n",
    "print(f\"{len(ds_train)} trajectories saved to train.npz.\")\n",
    "print(f\"{len(ds_valid)} trajectories saved to valid.npz.\")\n",
    "print(f\"{len(ds_test)}  trajectories saved to test.npz.\")\n",
    "\n",
    "# Save meta data\n",
    "in_file = '/home/jovyan/share/gns_data/Concrete2D-C/metadata.json'\n",
    "out_file = f'/home/jovyan/share/gns_data/{dataset}/metadata.json'\n",
    "\n",
    "with open(in_file, 'r') as f:\n",
    "    meta_data = json.load(f)\n",
    "\n",
    "# In GNN, the suggested connection radius is 4.5r, or 5.625 mm (aounrd 20 neighbors)\n",
    "# If R is 5 mm before normalization, \n",
    "meta_data['dim'] = 3\n",
    "meta_data['default_connectivity_radius'] = 11 \n",
    "meta_data['sequence_length'] = positions.shape[0]\n",
    "meta_data['vel_mean'] = vel_mean\n",
    "meta_data['vel_std'] = vel_std\n",
    "meta_data['acc_mean'] = acc_mean\n",
    "meta_data['acc_std'] = acc_std\n",
    "meta_data['strain_mean'] = strain_mean\n",
    "meta_data['strain_std'] = strain_std\n",
    "\n",
    "meta_data['dt'] = 0.0006 * STEP_SIZE\n",
    "meta_data['bounds'] = [[-500, 500], [-1000, 1000], [4, 124]]\n",
    "meta_data['file_train'] = file_train\n",
    "meta_data['file_valid'] = file_valid\n",
    "meta_data['file_test'] = file_test\n",
    "print(meta_data)\n",
    "\n",
    "# with open(out_file, 'w') as f:\n",
    "#     json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0db73d-5cca-40d6-b0f7-8398f3c825bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac9a2d2f-5d1e-4102-873b-be011b497add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "re_int_sci = r'[-\\d\\.]+e?[-+\\d]*'\n",
    "re_sci = r'[+-]?\\d+\\.\\d+e[+-]?[\\d]+'\n",
    "\n",
    "def parse_fragment_mps(file):\n",
    "    '''\n",
    "    Extract info from LSDYNA txt, including particle coordinates, particle types, and effective plastic strain (eps).\"\n",
    "    Input: Txt from LYDYNA, e.g., C_80_480_Cc_20_strain.txt\n",
    "    Output: np arrays of shapes, \n",
    "            tracjectory (timesteps, num_particles, 3), particle_type (num_particles,), eps (timesteps, num_particles).\n",
    "    '''\n",
    "                   \n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines() \n",
    "\n",
    "    # Find all \"particle position\" lines and \"plastic strain\" lines using key words\n",
    "    strain_lines_start, strain_lines_end = [], []\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"$RESULT OF  Max Prin Strain\"):\n",
    "            strain_lines_start.append(idx)\n",
    "        elif line.startswith(\"*END\"):\n",
    "            strain_lines_end.append(idx)\n",
    "\n",
    "    # Extract effective plastic strain (eps)\n",
    "    strains = []\n",
    "    for line_start, line_end in zip(strain_lines_start, strain_lines_end):\n",
    "        strain_lines = lines[line_start+1:line_end]   # lines that contains positions in one time step\n",
    "        strains_one_step = []\n",
    "        for line in strain_lines:\n",
    "            num_str = re.findall(re_sci, line)  # Regular expression findign scitific numbers\n",
    "            if len(num_str) == 1:\n",
    "                num = float(num_str[0]) \n",
    "                strains_one_step.append(num)\n",
    "        strains_one_step = [0]*6248 + strains_one_step   # Add zero strain to all beam particles\n",
    "        strains.append(strains_one_step)  \n",
    "    strains = np.array(strains).astype(float)\n",
    "    \n",
    "    return strains\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4486fc-e595-433f-bc11-4d1b1806f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 246248, 3) (246248,) (101, 246248)\n"
     ]
    }
   ],
   "source": [
    "file = '/home/jovyan/work/data_temp/fragment/120_4_0.3C50'\n",
    "trajectory, particle_types, strains = parse_fragment_simulation(file)\n",
    "print(trajectory.shape, particle_types.shape, strains.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966eb557-29f5-4f3b-86f4-fd42353df575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modify metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f182a3-c205-46ce-9c02-e13ba4635842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': [[-500, 500], [-1000, 1000], [0, 255]], 'sequence_length': 41, 'default_connectivity_radius': 11, 'dim': 3, 'dt': 0.0012, 'vel_mean': [0.00025711230813995186, 7.838677173801261e-05, 0.11218872474841513], 'vel_std': [0.04329208019487968, 0.050169923732292544, 0.21707507435839715], 'acc_mean': [4.228977435470556e-06, -1.0883637271305478e-05, -0.0007385390517020932], 'acc_std': [0.016808747817365178, 0.02908448767480537, 0.01776673986455495], 'file_train': ['/home/jovyan/work/data_temp/fragment/120/120_5_0.4C30', '/home/jovyan/work/data_temp/fragment/120/120_6_0.3C30', '/home/jovyan/work/data_temp/fragment/120/120_5_0.5C80', '/home/jovyan/work/data_temp/fragment/120/120_3_0.5C50', '/home/jovyan/work/data_temp/fragment/120/120_5_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_5_0.4C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.4C30', '/home/jovyan/work/data_temp/fragment/120/120_2_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_4_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_4_0.5C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.4C50', '/home/jovyan/work/data_temp/fragment/120/120_5_0.4C80', '/home/jovyan/work/data_temp/fragment/120/120_4_0.4C30', '/home/jovyan/work/data_temp/fragment/120/120_3_0.4C80', '/home/jovyan/work/data_temp/fragment/120/120_6_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.5C50', '/home/jovyan/work/data_temp/fragment/120/120_4_0.4C50', '/home/jovyan/work/data_temp/fragment/120/120_6_0.5C80', '/home/jovyan/work/data_temp/fragment/120/120_3_0.3C30', '/home/jovyan/work/data_temp/fragment/120/120_3_0.4C50', '/home/jovyan/work/data_temp/fragment/120/120_4_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_6_0.5C30', '/home/jovyan/work/data_temp/fragment/120/120_3_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_6_0.4C30', '/home/jovyan/work/data_temp/fragment/120/120_6_0.4C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.5C30', '/home/jovyan/work/data_temp/fragment/120/120_3_0.4C30', '/home/jovyan/work/data_temp/fragment/120/120_5_0.5C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_5_0.3C30', '/home/jovyan/work/data_temp/fragment/120/120_3_0.5C80', '/home/jovyan/work/data_temp/fragment/120/120_3_0.5C30', '/home/jovyan/work/data_temp/fragment/120/120_6_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_4_0.5C80', '/home/jovyan/work/data_temp/fragment/120/120_4_0.4C80', '/home/jovyan/work/data_temp/fragment/120/120_2_0.5C80', '/home/jovyan/work/data_temp/fragment/120/120_6_0.5C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.4C80', '/home/jovyan/work/data_temp/fragment/120/120_6_0.4C80', '/home/jovyan/work/data_temp/fragment/120/120_5_0.5C30'], 'file_valid': ['/home/jovyan/work/data_temp/fragment/120/120_4_0.3C30', '/home/jovyan/work/data_temp/fragment/120/120_4_0.5C30', '/home/jovyan/work/data_temp/fragment/120/120_5_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_3_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.3C30'], 'file_test': ['/home/jovyan/work/data_temp/fragment/120/120_4_0.3C30', '/home/jovyan/work/data_temp/fragment/120/120_4_0.5C30', '/home/jovyan/work/data_temp/fragment/120/120_5_0.3C80', '/home/jovyan/work/data_temp/fragment/120/120_3_0.3C50', '/home/jovyan/work/data_temp/fragment/120/120_2_0.3C30'], 'strain_mean': 0.8868453123315391, 'strain_std': 0.6590170029193022}\n"
     ]
    }
   ],
   "source": [
    "in_file = '/home/jovyan/share/gns_data/Fragment/metadata.json'\n",
    "out_file = f'/home/jovyan/work/data_temp/fragment/Fragment/metadata.json'\n",
    "\n",
    "with open(in_file, 'r') as f:\n",
    "    meta_data = json.load(f)\n",
    "\n",
    "meta_data['dim'] = 3\n",
    "meta_data['bounds'] = [[-500, 500], [-1000, 1000], [0, 255]]\n",
    "\n",
    "print(meta_data)\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6030fd9-c4e2-4e22-93c7-3c3e8671a802",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test regular expression for number extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d25e860-bb0a-4645-9e5a-2b2a08a8cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.4952594e+03', '-1.0499660e+02', '1.6313647e-02', '9.9995575e+02']\n",
      "['1.4051317e+00']\n",
      "[]\n",
      "['7.5878880e+07']\n",
      "['20742', '1.4952594e+03', '-1.0499660e+02', '1.6313647e-02', '9.9995575e+02']\n",
      "['32365', '1.4051317e+00']\n",
      "['10826', '1', '15757', '15758', '15784', '15783', '11311', '11312', '11338', '11337']\n",
      "['7.5878880e+07']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "strs = ['20742   1.4952594e+03   -1.0499660e+02   1.6313647e-02   9.9995575e+02',\n",
    "        '    32365   1.4051317e+00',\n",
    "        '   10826       1   15757   15758   15784   15783   11311   11312   11338   11337',\n",
    "        '$Total Solid element Volume =    7.5878880e+07'\n",
    "       ]\n",
    "\n",
    "pattern = r'[+-]?\\d+\\.\\d+e[+-]?[\\d]+'\n",
    "for str in strs:\n",
    "    print(re.findall(pattern, str))\n",
    "    \n",
    "pattern = r'[-\\d\\.]+e?[-+\\d]*'\n",
    "for str in strs:\n",
    "    print(re.findall(pattern, str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92542ac9-5dfe-4de9-87a2-4d715b9f7832",
   "metadata": {},
   "source": [
    "# Plot Fragment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb778a-0d02-4cf8-964a-62029ead6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "# Create a colormap\n",
    "cmap = plt.get_cmap(\"rainbow\")\n",
    "\n",
    "# Create a 3D scatter plot with custom figure size\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.set_box_aspect([1, 2, 0.1])\n",
    "\n",
    "# Initial scatter plot\n",
    "scatter = ax.scatter([], [], [], c=[], cmap=cmap, vmin=strains.min(), vmax=strains.max())\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(scatter)\n",
    "cbar.set_label(\"Strain\")\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "\n",
    "\n",
    "def update(timestep):\n",
    "    ax.clear()\n",
    "    ax.set_box_aspect([4, 8, 1])\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    \n",
    "    ax.set_title(f\"Time step: {timestep}\")\n",
    "\n",
    "    pos = position[timestep]\n",
    "    strains_t = strain[timestep]\n",
    "    scatter = ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2], c=strains_t, cmap=cmap)\n",
    "\n",
    "    return scatter,\n",
    "\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(strains.shape[0]), interval=200, blit=False)\n",
    "\n",
    "# Uncomment the following line to save the animation as a GIF\n",
    "ani.save(\"pred.gif\", writer=\"pillow\", fps=5, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7dd0ef-bcff-4e9f-a521-21810b57a479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 246248)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/home/jovyan/work/data_temp/fragment/Fragment/test.npz'\n",
    "data = [item for _, item in np.load(path, allow_pickle=True).items()]\n",
    "\n",
    "print(data[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c176cc0-325d-4c65-97fe-d569c36a5634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgn",
   "language": "python",
   "name": "mgn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
